# TPS Counter Implementation

This document describes how the Tokens Per Second (TPS) counter is implemented in this OpenCode fork.

## Overview

The TPS counter measures the actual generation speed of the LLM by tracking the duration of streaming parts (text and reasoning) and dividing the total tokens generated by that duration. This method provides a more accurate representation of model performance than simple "wall-clock" time, as it excludes network latency between parts and the execution time of tools.

## Implementation Details

### 1. Timing Capture (Server-side)

The server captures high-precision timestamps for every generation "part". This logic is located in the session processor.

**File:** `opencode/packages/opencode/src/session/processor.ts`

When the LLM starts and ends streaming a text or reasoning chunk, the processor records the current time:

```typescript
// Reasoning Parts
case "reasoning-start":
  reasoningMap[value.id] = {
    // ...
    time: { start: Date.now() },
  };
  break;

case "reasoning-end":
  const part = reasoningMap[value.id];
  part.time = { ...part.time, end: Date.now() };
  break;

// Text Parts
case "text-start":
  currentText = {
    // ...
    time: { start: Date.now() },
  };
  break;

case "text-end":
  currentText.time = { ...currentText.time, end: Date.now() };
  break;
```

### 2. TPS Calculation (Client-side)

The UI aggregates these timestamps and token counts to calculate the final TPS.

**File:** `opencode/packages/opencode/src/cli/cmd/tui/routes/session/index.tsx`

The calculation logic is encapsulated in a `createMemo`:

```typescript
const TPS = createMemo(() => {
  if (!final() || !props.message.time.completed) return 0;
  if (!Flag.OPENCODE_EXPERIMENTAL_TPS) return 0;

  const assistantMessages = messages().filter(
    (msg) => msg.role === "assistant" && msg.id !== props.message.id
  ) as AssistantMessage[];

  const allParts = assistantMessages.flatMap((msg) => getParts(msg.id));

  // Filter for streaming parts (reasoning + text) with valid timestamps
  const streamingParts = allParts.filter((part) => {
    return (part.type === "text" || part.type === "reasoning") && 
           part.time?.start && part.time?.end;
  });

  if (streamingParts.length === 0) return 0;

  // Sum individual part durations (excludes tool execution gaps)
  let totalStreamingTimeMs = 0;
  for (const part of streamingParts) {
    totalStreamingTimeMs += part.time!.end! - part.time!.start!;
  }

  if (totalStreamingTimeMs === 0) return 0;

  // Aggregate output and reasoning tokens
  const totals = assistantMessages.reduce(
    (acc, m) => {
      acc.output += m.tokens.output;
      acc.reasoning += m.tokens.reasoning;
      return acc;
    },
    { output: 0, reasoning: 0 }
  );

  const totalTokens = totals.reasoning + totals.output;
  const totalStreamingTimeSec = totalStreamingTimeMs / 1000;
  
  return Number((totalTokens / totalStreamingTimeSec).toFixed(2));
});
```

### 3. Feature Flag

The feature is guarded by an experimental flag, which can be enabled via the environment variable `OPENCODE_EXPERIMENTAL_TPS=true`.

**File:** `opencode/packages/opencode/src/flag/flag.ts`

```typescript
export const OPENCODE_EXPERIMENTAL_TPS = OPENCODE_EXPERIMENTAL || truthy("OPENCODE_EXPERIMENTAL_TPS");
```

## UI Usage

The calculated TPS is displayed in the assistant message footer:

```tsx
<Show when={Flag.OPENCODE_EXPERIMENTAL_TPS && TPS()}>
  <span style={{ fg: theme.textMuted }}> Â· {TPS()} tps</span>
</Show>
```
